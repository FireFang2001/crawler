### ç½‘ç»œçˆ¬è™«
ç½‘ç»œçˆ¬è™«æ˜¯ä¸€ç§æŒ‰ç…§ä¸€å®šçš„è§„åˆ™ï¼Œè‡ªåŠ¨åœ°æŠ“å–ä¸‡ç»´ç½‘ä¿¡æ¯çš„ç¨‹åºæˆ–è€…è„šæœ¬ã€‚
#### é€šç”¨çˆ¬è™«çš„ä¸€èˆ¬æ­¥éª¤
1.æŠ“å–é¡µé¢

2.è§£æé¡µé¢

3.æ•°æ®å­˜å‚¨


##### æŠ“å–é¡µé¢
ä¸‹è½½é¡µé¢æ•°æ®ç”¨åˆ°çš„åº“æœ‰urllib / requests / aiohttpç­‰ã€‚
ğŸŒ°æ —å­ï¼š
ä½¿ç”¨requestsçš„getæ–¹æ³•æŠ“å–çŸ¥ä¹é¦–é¡µ(requestsç”¨æ³•è¯·å‚è€ƒ[å®˜æ–¹æ–‡æ¡£](http://cn.python-requests.org/zh_CN/latest/))
```
url = 'https://www.zhihu.com/'  # ç½‘é¡µçš„url
headers = {'user-agent': 'Baiduspider'} è¯·æ±‚å¤´ 
proxies = {'http': 'http://122.114.31.177:808'}  # è®¾ç½®ä»£ç†
response = requests.get(url,
                headers=headers,
                proxies=proxies)
```

##### é¡µé¢è§£æ
åœ¨Pythonå¯ä»¥è°ƒç”¨å¾ˆå¤šèƒ½åšé¡µé¢è§£æçš„åº“ï¼š
re/lxml/ Beautiful Soup4(bs4)/pyquery
ğŸŒ°æ —å­ï¼š
ç”¨bs4+reè§£æé¡µé¢ï¼ˆBeautiful Soup4çš„ä½¿ç”¨å‚è§[å®˜æ–¹æ–‡æ¡£](https://www.crummy.com/software/BeautifulSoup/bs4/doc/index.zh.html)ï¼‰
```
soup = BeautifulSoup(response.text, 'lxml')
regex = re.compile(r'^/question')
for a in soup.find_all('a', {'href': regex}):
    if 'href' in a.attrs:
        path = a_tag.attrs['href']
```

##### æ•°æ®å­˜å‚¨
çˆ¬è™«çš„æœ€ç»ˆç›®çš„æ˜¯ä»ç½‘ç»œä¸Šè·å–æ•°æ®ï¼Œè¿™å°±éœ€è¦åšæ•°æ®æŒä¹…åŒ–çš„å·¥ä½œäº†ã€‚
è¿™é‡Œæˆ‘ä»¬åŒæ ·æœ‰å¤šç§é€‰æ‹©ï¼š
MySQL/Redis/MongoDB
ä¸€äº›ORMï¼špymysql/sqlalchemy/peewee/redis/pymongo
åœ¨ä¸¾ä¸ªğŸŒ°æ —å­ï¼š
```
client = pymongo.MongoClient('47.98.56.23', 27017)
pages = client.zhihu.pages
pages.insert({'_id': page_id,
              'html_page': html_page,})    
```
